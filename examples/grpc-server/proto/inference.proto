syntax = "proto3";

package trustformers.inference;

import "google/protobuf/struct.proto";
import "google/protobuf/empty.proto";

// Main inference service for TrustformeRS models
service InferenceService {
  // Single inference request
  rpc Predict(PredictRequest) returns (PredictResponse);
  
  // Streaming inference for real-time responses
  rpc StreamPredict(stream StreamPredictRequest) returns (stream StreamPredictResponse);
  
  // Batch inference for multiple inputs
  rpc BatchPredict(BatchPredictRequest) returns (BatchPredictResponse);
  
  // Get available models
  rpc ListModels(google.protobuf.Empty) returns (ListModelsResponse);
  
  // Get model information
  rpc GetModelInfo(GetModelInfoRequest) returns (ModelInfo);
  
  // Load a model into memory
  rpc LoadModel(LoadModelRequest) returns (LoadModelResponse);
  
  // Unload a model from memory
  rpc UnloadModel(UnloadModelRequest) returns (google.protobuf.Empty);
}

// Request for single prediction
message PredictRequest {
  string model_id = 1;
  oneof input {
    string text = 2;
    TextInput text_input = 3;
    ImageInput image_input = 4;
  }
  PredictOptions options = 5;
}

// Text input with optional parameters
message TextInput {
  string text = 1;
  int32 max_length = 2;
  bool add_special_tokens = 3;
  bool return_tensors = 4;
}

// Image input
message ImageInput {
  bytes data = 1;
  string format = 2; // "png", "jpeg", etc.
  repeated int32 size = 3; // [height, width]
}

// Prediction options
message PredictOptions {
  float temperature = 1;
  int32 top_k = 2;
  float top_p = 3;
  int32 max_new_tokens = 4;
  bool do_sample = 5;
  int32 num_beams = 6;
  float repetition_penalty = 7;
  repeated string stop_sequences = 8;
  int32 seed = 9;
  bool use_cache = 10;
  string device = 11; // "cpu", "cuda", "metal"
  bool use_fp16 = 12;
  google.protobuf.Struct extra_params = 13;
}

// Response for single prediction
message PredictResponse {
  oneof output {
    TextOutput text_output = 1;
    ClassificationOutput classification_output = 2;
    TokenClassificationOutput token_classification_output = 3;
    QuestionAnsweringOutput qa_output = 4;
    GenerationOutput generation_output = 5;
    EmbeddingOutput embedding_output = 6;
  }
  PredictMetrics metrics = 7;
}

// Text generation output
message TextOutput {
  string text = 1;
  repeated string texts = 2; // For multiple sequences
  repeated float scores = 3;
  repeated int32 token_ids = 4;
}

// Classification output
message ClassificationOutput {
  repeated Label labels = 1;
  repeated float scores = 2;
  int32 predicted_class = 3;
}

// Token classification output (e.g., NER)
message TokenClassificationOutput {
  repeated TokenLabel tokens = 1;
}

// Question answering output
message QuestionAnsweringOutput {
  string answer = 1;
  float score = 2;
  int32 start_index = 3;
  int32 end_index = 4;
}

// Generation output with detailed information
message GenerationOutput {
  repeated Sequence sequences = 1;
  repeated float scores = 2;
  repeated Attention attentions = 3;
}

// Embedding output
message EmbeddingOutput {
  repeated float embeddings = 1;
  repeated int32 shape = 2;
}

// Label with score
message Label {
  string label = 1;
  float score = 2;
}

// Token with label
message TokenLabel {
  string token = 1;
  string label = 2;
  float score = 3;
  int32 start = 4;
  int32 end = 5;
}

// Generated sequence
message Sequence {
  string text = 1;
  repeated int32 token_ids = 2;
  float score = 3;
}

// Attention weights (optional)
message Attention {
  repeated float weights = 1;
  repeated int32 shape = 2;
}

// Metrics for the prediction
message PredictMetrics {
  float latency_ms = 1;
  int32 tokens_per_second = 2;
  int64 memory_used_bytes = 3;
  string device_used = 4;
}

// Streaming request
message StreamPredictRequest {
  oneof request {
    StreamStartRequest start = 1;
    StreamContinueRequest continue = 2;
  }
}

// Start a streaming session
message StreamStartRequest {
  string model_id = 1;
  string initial_text = 2;
  PredictOptions options = 3;
  string session_id = 4;
}

// Continue streaming
message StreamContinueRequest {
  string session_id = 1;
  string text = 2;
  bool end_stream = 3;
}

// Streaming response
message StreamPredictResponse {
  string session_id = 1;
  string text = 2;
  repeated int32 token_ids = 3;
  bool is_final = 4;
  PredictMetrics metrics = 5;
}

// Batch prediction request
message BatchPredictRequest {
  string model_id = 1;
  repeated string texts = 2;
  PredictOptions options = 3;
  int32 batch_size = 4;
}

// Batch prediction response
message BatchPredictResponse {
  repeated PredictResponse predictions = 1;
  BatchMetrics metrics = 2;
}

// Batch processing metrics
message BatchMetrics {
  float total_latency_ms = 1;
  float avg_latency_ms = 2;
  int32 total_tokens = 3;
  float tokens_per_second = 4;
  int32 batch_size = 5;
}

// Model information
message ModelInfo {
  string model_id = 1;
  string model_type = 2;
  string architecture = 3;
  int64 num_parameters = 4;
  repeated string supported_tasks = 5;
  ModelConfig config = 6;
  ModelStatus status = 7;
  map<string, string> metadata = 8;
}

// Model configuration
message ModelConfig {
  int32 hidden_size = 1;
  int32 num_layers = 2;
  int32 num_heads = 3;
  int32 vocab_size = 4;
  int32 max_position_embeddings = 5;
  string model_type = 6;
  google.protobuf.Struct extra_config = 7;
}

// Model status
message ModelStatus {
  bool is_loaded = 1;
  string device = 2;
  int64 memory_used_bytes = 3;
  string load_time = 4;
  int32 request_count = 5;
}

// List models response
message ListModelsResponse {
  repeated ModelInfo models = 1;
}

// Get model info request
message GetModelInfoRequest {
  string model_id = 1;
}

// Load model request
message LoadModelRequest {
  string model_id = 1;
  string model_path = 2;
  string device = 3;
  bool use_fp16 = 4;
  bool compile = 5;
  map<string, string> options = 6;
}

// Load model response
message LoadModelResponse {
  string model_id = 1;
  bool success = 2;
  string message = 3;
  float load_time_ms = 4;
}

// Unload model request
message UnloadModelRequest {
  string model_id = 1;
}