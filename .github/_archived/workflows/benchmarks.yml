name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    types: [ opened, synchronize ]
  workflow_dispatch:
    inputs:
      comparison_branch:
        description: 'Branch to compare against'
        required: false
        default: 'main'

env:
  RUST_BACKTRACE: 1
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
      
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libssl-dev pkg-config cmake
          cargo install cargo-criterion critcmp
      
      - name: Cache cargo
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target/release
            target/criterion
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
      
      - name: Run benchmarks on current branch
        run: |
          # Run all benchmark suites
          cargo criterion --message-format=json > current-benchmarks.json || true
          
          # Run individual benchmarks for detailed results
          cargo criterion --bench tensor_ops_bench -- --save-baseline current || true
          cargo criterion --bench model_inference_bench -- --save-baseline current || true
          cargo criterion --bench optimizer_bench -- --save-baseline current || true
          cargo criterion --bench tokenizer_bench -- --save-baseline current || true
          cargo criterion --bench quantization_bench -- --save-baseline current || true
          cargo criterion --bench memory_bench -- --save-baseline current || true
          
          cp -r target/criterion current-criterion
      
      - name: Checkout comparison branch
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            git checkout ${{ github.base_ref }}
          else
            git checkout ${{ github.event.inputs.comparison_branch || 'main' }}~1
          fi
      
      - name: Run benchmarks on comparison branch
        run: |
          # Run all benchmark suites for baseline
          cargo criterion --message-format=json > base-benchmarks.json || true
          
          # Run individual benchmarks for detailed results
          cargo criterion --bench tensor_ops_bench -- --save-baseline base || true
          cargo criterion --bench model_inference_bench -- --save-baseline base || true
          cargo criterion --bench optimizer_bench -- --save-baseline base || true
          cargo criterion --bench tokenizer_bench -- --save-baseline base || true
          cargo criterion --bench quantization_bench -- --save-baseline base || true
          cargo criterion --bench memory_bench -- --save-baseline base || true
          
          cp -r target/criterion base-criterion
      
      - name: Compare benchmarks
        id: compare
        run: |
          # Generate comparison report using critcmp
          critcmp base-criterion current-criterion > comparison.txt
          
          # Run regression detection script
          python3 scripts/regression_detection.py \
            --current current-benchmarks.json \
            --baseline base-benchmarks.json \
            --config regression_config.json \
            --output markdown \
            --output-file regression-report.md || true
          
          # Create combined report
          cat > benchmark-report.md << 'EOF'
          ## Benchmark Results
          
          ### Summary
          Comparing performance between branches.
          
          ### Critcmp Results
          ```
          EOF
          cat comparison.txt >> benchmark-report.md
          echo '```' >> benchmark-report.md
          echo "" >> benchmark-report.md
          
          # Add regression detection report
          if [ -f regression-report.md ]; then
            cat regression-report.md >> benchmark-report.md
          fi
          
          # Check for regressions
          if python3 scripts/regression_detection.py \
            --current current-benchmarks.json \
            --baseline base-benchmarks.json \
            --config regression_config.json \
            --output json | jq -e '.errors | length > 0' > /dev/null 2>&1; then
            echo "has_regression=true" >> $GITHUB_OUTPUT
            echo "::error::Critical performance regression detected"
          elif grep -E "([0-9]+\.[0-9]+%|\+[0-9]+%) slower" comparison.txt; then
            echo "has_regression=true" >> $GITHUB_OUTPUT
            echo "::warning::Performance regression detected"
          else
            echo "has_regression=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Post results to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('benchmark-report.md', 'utf8');
            
            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('## Benchmark Results')
            );
            
            const body = `${report}\n\n<sub>Generated by workflow run [#${context.runNumber}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})</sub>`;
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
      
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: |
            current-benchmarks.json
            base-benchmarks.json
            comparison.txt
            benchmark-report.md
      
      - name: Fail if regression
        if: steps.compare.outputs.has_regression == 'true' && github.event_name == 'pull_request'
        run: |
          echo "::error::Performance regression detected. See benchmark report for details."
          exit 1

  profile-guided-benchmarks:
    name: Profile-Guided Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
      
      - name: Install profiling tools
        run: |
          sudo apt-get update
          sudo apt-get install -y linux-tools-common linux-tools-generic
          cargo install flamegraph
      
      - name: Run profiled benchmarks
        run: |
          # Enable perf events
          echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
          
          # Run benchmarks with profiling
          cargo flamegraph --bench inference_bench -- --bench
      
      - name: Upload flamegraph
        uses: actions/upload-artifact@v3
        with:
          name: flamegraph
          path: flamegraph.svg

  memory-benchmarks:
    name: Memory Usage Benchmarks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
      
      - name: Install heaptrack
        run: |
          sudo apt-get update
          sudo apt-get install -y heaptrack
      
      - name: Build benchmarks
        run: cargo build --release --benches
      
      - name: Run memory benchmarks
        run: |
          # Find all benchmark binaries
          for bench in target/release/deps/*bench*; do
            if [ -x "$bench" ] && [ -f "$bench" ]; then
              echo "Running memory benchmark for: $bench"
              heaptrack "$bench" --bench || true
            fi
          done
      
      - name: Analyze memory usage
        run: |
          for heap in heaptrack.*.gz; do
            if [ -f "$heap" ]; then
              heaptrack --analyze "$heap" > "${heap%.gz}.txt"
            fi
          done
      
      - name: Upload memory analysis
        uses: actions/upload-artifact@v3
        with:
          name: memory-analysis
          path: heaptrack.*.txt