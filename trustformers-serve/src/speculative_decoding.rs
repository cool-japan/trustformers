/// Result type for speculative decoding operations
pub type Result<T> = std::result::Result<T, ServingError>;

/// Error types for speculative decoding
#[derive(Debug, thiserror::Error)]
pub enum ServingError {
    #[error("Draft model inference failed: {0}")]
    DraftModelError(String),

    #[error("Target model inference failed: {0}")]
    TargetModelError(String),

    #[error("Token verification failed: {0}")]
    VerificationError(String),

    #[error("Timeout occurred: {0}")]
    TimeoutError(String),

    #[error("Configuration error: {0}")]
    ConfigError(String),

    #[error("Internal error: {0}")]
    InternalError(String),
}
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::VecDeque;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tokio::sync::RwLock;

/// Speculative decoding engine for accelerated inference
///
/// This module implements speculative decoding, an inference optimization technique
/// that uses a smaller, faster "draft" model to generate candidate tokens, which are
/// then verified by the larger "target" model. This can significantly improve latency
/// while maintaining the quality of the larger model.
///
/// Key features:
/// - Parallel execution of draft and target models
/// - Adaptive speculation length based on acceptance rates
/// - Token verification with rejection sampling
/// - Performance monitoring and statistics
/// - Fallback to standard autoregressive decoding

/// Configuration for speculative decoding
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SpeculativeDecodingConfig {
    /// Maximum number of tokens to speculate ahead
    pub max_speculation_length: usize,
    /// Minimum speculation length to maintain
    pub min_speculation_length: usize,
    /// Target acceptance rate for adaptive speculation
    pub target_acceptance_rate: f32,
    /// Window size for tracking acceptance rate
    pub acceptance_rate_window: usize,
    /// Temperature for rejection sampling
    pub temperature: f32,
    /// Whether to use adaptive speculation length
    pub adaptive_speculation: bool,
    /// Timeout for draft model inference (milliseconds)
    pub draft_timeout_ms: u64,
    /// Timeout for target model inference (milliseconds)
    pub target_timeout_ms: u64,
    /// Whether to enable parallel execution
    pub parallel_execution: bool,
    /// Whether to enable verification caching
    pub enable_verification_cache: bool,
    /// Maximum cache size for verification results
    pub verification_cache_size: usize,
}

impl Default for SpeculativeDecodingConfig {
    fn default() -> Self {
        Self {
            max_speculation_length: 8,
            min_speculation_length: 2,
            target_acceptance_rate: 0.7,
            acceptance_rate_window: 100,
            temperature: 1.0,
            adaptive_speculation: true,
            draft_timeout_ms: 50,
            target_timeout_ms: 200,
            parallel_execution: true,
            enable_verification_cache: true,
            verification_cache_size: 10000,
        }
    }
}

/// Token generated by the draft model with associated probability
#[derive(Debug, Clone)]
pub struct DraftToken {
    pub token_id: u32,
    pub logit: f32,
    pub probability: f32,
    pub timestamp: Instant,
}

/// Result of target model verification
#[derive(Debug, Clone)]
pub struct VerificationResult {
    pub accepted_count: usize,
    pub rejected_position: Option<usize>,
    pub target_logits: Vec<f32>,
    pub accepted_tokens: Vec<u32>,
    pub correction_token: Option<u32>,
}

/// Statistics for speculative decoding performance
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SpeculativeStats {
    pub total_sequences: u64,
    pub total_draft_tokens: u64,
    pub total_accepted_tokens: u64,
    pub total_rejected_tokens: u64,
    pub average_acceptance_rate: f32,
    pub current_speculation_length: usize,
    pub draft_model_latency_ms: f32,
    pub target_model_latency_ms: f32,
    pub total_latency_savings_ms: f32,
    pub cache_hits: u64,
    pub cache_misses: u64,
    pub adaptive_adjustments: u64,
}

impl Default for SpeculativeStats {
    fn default() -> Self {
        Self {
            total_sequences: 0,
            total_draft_tokens: 0,
            total_accepted_tokens: 0,
            total_rejected_tokens: 0,
            average_acceptance_rate: 0.0,
            current_speculation_length: 4,
            draft_model_latency_ms: 0.0,
            target_model_latency_ms: 0.0,
            total_latency_savings_ms: 0.0,
            cache_hits: 0,
            cache_misses: 0,
            adaptive_adjustments: 0,
        }
    }
}

/// Cache entry for verification results
#[derive(Debug, Clone)]
struct VerificationCacheEntry {
    context_hash: u64,
    draft_tokens: Vec<u32>,
    verification_result: VerificationResult,
    timestamp: Instant,
}

/// Speculative decoding engine
pub struct SpeculativeDecoder {
    config: SpeculativeDecodingConfig,
    stats: Arc<RwLock<SpeculativeStats>>,
    acceptance_history: Arc<Mutex<VecDeque<bool>>>,
    current_speculation_length: Arc<Mutex<usize>>,
    verification_cache: Arc<Mutex<VecDeque<VerificationCacheEntry>>>,
}

impl SpeculativeDecoder {
    /// Create a new speculative decoder
    pub fn new(config: SpeculativeDecodingConfig) -> Self {
        let initial_speculation_length =
            (config.max_speculation_length + config.min_speculation_length) / 2;
        let acceptance_rate_window = config.acceptance_rate_window;
        let verification_cache_size = config.verification_cache_size;

        Self {
            config,
            stats: Arc::new(RwLock::new(SpeculativeStats {
                current_speculation_length: initial_speculation_length,
                ..Default::default()
            })),
            acceptance_history: Arc::new(Mutex::new(VecDeque::with_capacity(
                acceptance_rate_window,
            ))),
            current_speculation_length: Arc::new(Mutex::new(initial_speculation_length)),
            verification_cache: Arc::new(Mutex::new(VecDeque::with_capacity(
                verification_cache_size,
            ))),
        }
    }

    /// Generate tokens using speculative decoding
    pub async fn generate(
        &self,
        context_tokens: &[u32],
        max_new_tokens: usize,
        draft_model: &dyn DraftModel,
        target_model: &dyn TargetModel,
    ) -> Result<Vec<u32>> {
        let start_time = Instant::now();
        let mut generated_tokens = Vec::new();
        let mut current_context = context_tokens.to_vec();

        while generated_tokens.len() < max_new_tokens {
            let remaining_tokens = max_new_tokens - generated_tokens.len();
            let speculation_length =
                std::cmp::min(self.get_current_speculation_length(), remaining_tokens);

            // Step 1: Generate draft tokens
            let draft_start = Instant::now();
            let draft_tokens = self
                .generate_draft_tokens(&current_context, speculation_length, draft_model)
                .await?;
            let draft_latency = draft_start.elapsed();

            if draft_tokens.is_empty() {
                // Fallback to standard autoregressive decoding
                let single_token = target_model.generate_next_token(&current_context).await?;
                generated_tokens.push(single_token);
                current_context.push(single_token);
                continue;
            }

            // Step 2: Verify draft tokens with target model
            let target_start = Instant::now();
            let verification_result =
                self.verify_tokens(&current_context, &draft_tokens, target_model).await?;
            let target_latency = target_start.elapsed();

            // Step 3: Process verification results
            let accepted_tokens = &verification_result.accepted_tokens;
            generated_tokens.extend_from_slice(accepted_tokens);
            current_context.extend_from_slice(accepted_tokens);

            // Add correction token if rejection occurred
            if let Some(correction_token) = verification_result.correction_token {
                generated_tokens.push(correction_token);
                current_context.push(correction_token);
            }

            // Step 4: Update statistics and adaptive parameters
            self.update_stats(
                &draft_tokens,
                &verification_result,
                draft_latency,
                target_latency,
            )
            .await;

            // If we got fewer tokens than expected, we might be at end of sequence
            if accepted_tokens.len() < draft_tokens.len()
                && verification_result.correction_token.is_none()
            {
                break;
            }
        }

        let total_latency = start_time.elapsed();
        self.update_sequence_stats(total_latency).await;

        Ok(generated_tokens)
    }

    /// Generate tokens using the draft model
    async fn generate_draft_tokens(
        &self,
        context: &[u32],
        length: usize,
        draft_model: &dyn DraftModel,
    ) -> Result<Vec<DraftToken>> {
        let timeout = Duration::from_millis(self.config.draft_timeout_ms);

        match tokio::time::timeout(timeout, draft_model.generate_tokens(context, length)).await {
            Ok(Ok(tokens)) => Ok(tokens),
            Ok(Err(e)) => Err(e),
            Err(_) => {
                // Timeout occurred - return empty to fallback to autoregressive
                Ok(Vec::new())
            },
        }
    }

    /// Verify draft tokens using the target model
    async fn verify_tokens(
        &self,
        context: &[u32],
        draft_tokens: &[DraftToken],
        target_model: &dyn TargetModel,
    ) -> Result<VerificationResult> {
        // Check cache first if enabled
        if self.config.enable_verification_cache {
            if let Some(cached_result) = self.check_verification_cache(context, draft_tokens).await
            {
                self.update_cache_stats(true).await;
                return Ok(cached_result);
            }
            self.update_cache_stats(false).await;
        }

        let timeout = Duration::from_millis(self.config.target_timeout_ms);

        let result = match tokio::time::timeout(
            timeout,
            target_model.verify_tokens(context, draft_tokens, self.config.temperature),
        )
        .await
        {
            Ok(Ok(result)) => result,
            Ok(Err(e)) => return Err(e),
            Err(_) => {
                // Timeout - reject all tokens and generate one new token
                let new_token = target_model.generate_next_token(context).await?;
                VerificationResult {
                    accepted_count: 0,
                    rejected_position: Some(0),
                    target_logits: Vec::new(),
                    accepted_tokens: Vec::new(),
                    correction_token: Some(new_token),
                }
            },
        };

        // Cache the result if enabled
        if self.config.enable_verification_cache {
            self.cache_verification_result(context, draft_tokens, &result).await;
        }

        Ok(result)
    }

    /// Check if verification result is cached
    async fn check_verification_cache(
        &self,
        context: &[u32],
        draft_tokens: &[DraftToken],
    ) -> Option<VerificationResult> {
        let context_hash = self.compute_context_hash(context);
        let draft_token_ids: Vec<u32> = draft_tokens.iter().map(|t| t.token_id).collect();

        let cache = self.verification_cache.lock().expect("lock should not be poisoned");
        for entry in cache.iter() {
            if entry.context_hash == context_hash && entry.draft_tokens == draft_token_ids {
                // Check if cache entry is still fresh (within 1 minute)
                if entry.timestamp.elapsed() < Duration::from_secs(60) {
                    return Some(entry.verification_result.clone());
                }
            }
        }

        None
    }

    /// Cache verification result
    async fn cache_verification_result(
        &self,
        context: &[u32],
        draft_tokens: &[DraftToken],
        result: &VerificationResult,
    ) {
        let mut cache = self.verification_cache.lock().expect("lock should not be poisoned");

        let entry = VerificationCacheEntry {
            context_hash: self.compute_context_hash(context),
            draft_tokens: draft_tokens.iter().map(|t| t.token_id).collect(),
            verification_result: result.clone(),
            timestamp: Instant::now(),
        };

        cache.push_back(entry);

        // Maintain cache size limit
        while cache.len() > self.config.verification_cache_size {
            cache.pop_front();
        }
    }

    /// Compute hash for context tokens
    fn compute_context_hash(&self, context: &[u32]) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();

        // Hash last 32 tokens for context locality
        let start_idx = context.len().saturating_sub(32);
        context[start_idx..].hash(&mut hasher);

        hasher.finish()
    }

    /// Update statistics after token verification
    async fn update_stats(
        &self,
        draft_tokens: &[DraftToken],
        verification_result: &VerificationResult,
        draft_latency: Duration,
        target_latency: Duration,
    ) {
        let mut stats = self.stats.write().await;

        stats.total_draft_tokens += draft_tokens.len() as u64;
        stats.total_accepted_tokens += verification_result.accepted_count as u64;
        stats.total_rejected_tokens +=
            (draft_tokens.len() - verification_result.accepted_count) as u64;

        // Update latency statistics
        stats.draft_model_latency_ms =
            (stats.draft_model_latency_ms * 0.9) + (draft_latency.as_millis() as f32 * 0.1);
        stats.target_model_latency_ms =
            (stats.target_model_latency_ms * 0.9) + (target_latency.as_millis() as f32 * 0.1);

        // Calculate acceptance rate
        let acceptance_rate = verification_result.accepted_count as f32 / draft_tokens.len() as f32;

        // Update acceptance history for adaptive speculation
        if self.config.adaptive_speculation {
            let mut history = self.acceptance_history.lock().expect("lock should not be poisoned");
            history.push_back(acceptance_rate > 0.5);

            if history.len() > self.config.acceptance_rate_window {
                history.pop_front();
            }

            // Adjust speculation length if we have enough history
            if history.len() >= self.config.acceptance_rate_window / 2 {
                let current_rate =
                    history.iter().filter(|&&x| x).count() as f32 / history.len() as f32;
                stats.average_acceptance_rate = current_rate;

                let mut current_length =
                    self.current_speculation_length.lock().expect("lock should not be poisoned");

                if current_rate > self.config.target_acceptance_rate + 0.1 {
                    // Acceptance rate is high, try increasing speculation length
                    if *current_length < self.config.max_speculation_length {
                        *current_length += 1;
                        stats.adaptive_adjustments += 1;
                    }
                } else if current_rate < self.config.target_acceptance_rate - 0.1 {
                    // Acceptance rate is low, decrease speculation length
                    if *current_length > self.config.min_speculation_length {
                        *current_length -= 1;
                        stats.adaptive_adjustments += 1;
                    }
                }

                stats.current_speculation_length = *current_length;
            }
        }
    }

    /// Update cache statistics
    async fn update_cache_stats(&self, cache_hit: bool) {
        let mut stats = self.stats.write().await;
        if cache_hit {
            stats.cache_hits += 1;
        } else {
            stats.cache_misses += 1;
        }
    }

    /// Update sequence-level statistics
    async fn update_sequence_stats(&self, _total_latency: Duration) {
        let mut stats = self.stats.write().await;
        stats.total_sequences += 1;

        // Estimate latency savings based on acceptance rate
        let potential_savings = stats.average_acceptance_rate * stats.draft_model_latency_ms;
        stats.total_latency_savings_ms += potential_savings;
    }

    /// Get current speculation length
    fn get_current_speculation_length(&self) -> usize {
        *self.current_speculation_length.lock().expect("lock should not be poisoned")
    }

    /// Get current statistics
    pub async fn get_stats(&self) -> SpeculativeStats {
        self.stats.read().await.clone()
    }

    /// Reset statistics
    pub async fn reset_stats(&self) {
        let mut stats = self.stats.write().await;
        *stats = SpeculativeStats::default();

        let mut history = self.acceptance_history.lock().expect("lock should not be poisoned");
        history.clear();

        let mut cache = self.verification_cache.lock().expect("lock should not be poisoned");
        cache.clear();
    }

    /// Update configuration
    pub async fn update_config(&self, _new_config: SpeculativeDecodingConfig) {
        // Note: In a real implementation, we would need to handle config updates more carefully
        // For now, this is a placeholder for the interface
    }
}

/// Trait for draft models used in speculative decoding
#[async_trait]
pub trait DraftModel: Send + Sync {
    /// Generate multiple tokens autoregressively
    async fn generate_tokens(&self, context: &[u32], num_tokens: usize) -> Result<Vec<DraftToken>>;

    /// Get model information
    fn model_info(&self) -> ModelInfo;
}

/// Trait for target models used in speculative decoding
#[async_trait]
pub trait TargetModel: Send + Sync {
    /// Verify draft tokens and return verification result
    async fn verify_tokens(
        &self,
        context: &[u32],
        draft_tokens: &[DraftToken],
        temperature: f32,
    ) -> Result<VerificationResult>;

    /// Generate a single next token (fallback)
    async fn generate_next_token(&self, context: &[u32]) -> Result<u32>;

    /// Get model information
    fn model_info(&self) -> ModelInfo;
}

/// Model information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelInfo {
    pub name: String,
    pub size: String,
    pub parameters: u64,
    pub vocab_size: u32,
    pub context_length: usize,
}

/// Manager for speculative decoding sessions
pub struct SpeculativeDecodingManager {
    decoders: Arc<RwLock<std::collections::HashMap<String, Arc<SpeculativeDecoder>>>>,
    default_config: SpeculativeDecodingConfig,
}

impl SpeculativeDecodingManager {
    /// Create a new manager
    pub fn new(default_config: SpeculativeDecodingConfig) -> Self {
        Self {
            decoders: Arc::new(RwLock::new(std::collections::HashMap::new())),
            default_config,
        }
    }

    /// Get or create a decoder for a session
    pub async fn get_decoder(&self, session_id: &str) -> Arc<SpeculativeDecoder> {
        let decoders = self.decoders.read().await;

        if let Some(decoder) = decoders.get(session_id) {
            decoder.clone()
        } else {
            drop(decoders);

            let mut decoders = self.decoders.write().await;

            // Double-check after acquiring write lock
            if let Some(decoder) = decoders.get(session_id) {
                decoder.clone()
            } else {
                let decoder = Arc::new(SpeculativeDecoder::new(self.default_config.clone()));
                decoders.insert(session_id.to_string(), decoder.clone());
                decoder
            }
        }
    }

    /// Remove a decoder session
    pub async fn remove_decoder(&self, session_id: &str) -> Option<Arc<SpeculativeDecoder>> {
        let mut decoders = self.decoders.write().await;
        decoders.remove(session_id)
    }

    /// Get statistics for all sessions
    pub async fn get_all_stats(&self) -> std::collections::HashMap<String, SpeculativeStats> {
        let decoders = self.decoders.read().await;
        let mut all_stats = std::collections::HashMap::new();

        for (session_id, decoder) in decoders.iter() {
            let stats = decoder.get_stats().await;
            all_stats.insert(session_id.clone(), stats);
        }

        all_stats
    }

    /// Get aggregate statistics across all sessions
    pub async fn get_aggregate_stats(&self) -> SpeculativeStats {
        let all_stats = self.get_all_stats().await;

        if all_stats.is_empty() {
            return SpeculativeStats::default();
        }

        let mut aggregate = SpeculativeStats::default();
        let count = all_stats.len() as f32;

        for stats in all_stats.values() {
            aggregate.total_sequences += stats.total_sequences;
            aggregate.total_draft_tokens += stats.total_draft_tokens;
            aggregate.total_accepted_tokens += stats.total_accepted_tokens;
            aggregate.total_rejected_tokens += stats.total_rejected_tokens;
            aggregate.average_acceptance_rate += stats.average_acceptance_rate / count;
            aggregate.draft_model_latency_ms += stats.draft_model_latency_ms / count;
            aggregate.target_model_latency_ms += stats.target_model_latency_ms / count;
            aggregate.total_latency_savings_ms += stats.total_latency_savings_ms;
            aggregate.cache_hits += stats.cache_hits;
            aggregate.cache_misses += stats.cache_misses;
            aggregate.adaptive_adjustments += stats.adaptive_adjustments;
        }

        aggregate
    }

    /// Clean up inactive sessions
    pub async fn cleanup_inactive_sessions(&self, _max_idle_time: Duration) {
        let mut decoders = self.decoders.write().await;
        let mut to_remove = Vec::new();

        for (session_id, _) in decoders.iter() {
            // In a real implementation, we would track last access time
            // For now, this is a placeholder
            to_remove.push(session_id.clone());
        }

        // Remove sessions that haven't been active
        for session_id in to_remove {
            decoders.remove(&session_id);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tokio;

    /// Mock draft model for testing
    struct MockDraftModel {
        tokens_to_generate: Vec<u32>,
    }

    impl MockDraftModel {
        fn new(tokens: Vec<u32>) -> Self {
            Self {
                tokens_to_generate: tokens,
            }
        }
    }

    #[async_trait::async_trait]
    impl DraftModel for MockDraftModel {
        async fn generate_tokens(
            &self,
            _context: &[u32],
            num_tokens: usize,
        ) -> Result<Vec<DraftToken>> {
            let mut tokens = Vec::new();
            for i in 0..std::cmp::min(num_tokens, self.tokens_to_generate.len()) {
                tokens.push(DraftToken {
                    token_id: self.tokens_to_generate[i],
                    logit: 2.0,
                    probability: 0.8,
                    timestamp: Instant::now(),
                });
            }
            Ok(tokens)
        }

        fn model_info(&self) -> ModelInfo {
            ModelInfo {
                name: "MockDraft".to_string(),
                size: "small".to_string(),
                parameters: 1_000_000,
                vocab_size: 32000,
                context_length: 2048,
            }
        }
    }

    /// Mock target model for testing
    struct MockTargetModel {
        acceptance_rate: f32,
    }

    impl MockTargetModel {
        fn new(acceptance_rate: f32) -> Self {
            Self { acceptance_rate }
        }
    }

    #[async_trait::async_trait]
    impl TargetModel for MockTargetModel {
        async fn verify_tokens(
            &self,
            _context: &[u32],
            draft_tokens: &[DraftToken],
            _temperature: f32,
        ) -> Result<VerificationResult> {
            let accepted_count = (draft_tokens.len() as f32 * self.acceptance_rate) as usize;
            let accepted_tokens: Vec<u32> =
                draft_tokens[..accepted_count].iter().map(|t| t.token_id).collect();

            let correction_token = if accepted_count < draft_tokens.len() {
                Some(9999) // Mock correction token
            } else {
                None
            };

            Ok(VerificationResult {
                accepted_count,
                rejected_position: if accepted_count < draft_tokens.len() {
                    Some(accepted_count)
                } else {
                    None
                },
                target_logits: vec![1.0; 32000],
                accepted_tokens,
                correction_token,
            })
        }

        async fn generate_next_token(&self, _context: &[u32]) -> Result<u32> {
            Ok(9999) // Mock single token
        }

        fn model_info(&self) -> ModelInfo {
            ModelInfo {
                name: "MockTarget".to_string(),
                size: "large".to_string(),
                parameters: 70_000_000_000,
                vocab_size: 32000,
                context_length: 4096,
            }
        }
    }

    #[tokio::test]
    async fn test_speculative_decoder_creation() {
        let config = SpeculativeDecodingConfig::default();
        let decoder = SpeculativeDecoder::new(config);

        let stats = decoder.get_stats().await;
        assert_eq!(stats.total_sequences, 0);
        assert_eq!(stats.total_draft_tokens, 0);
        assert_eq!(stats.total_accepted_tokens, 0);
    }

    #[tokio::test]
    async fn test_speculative_decoding_generation() {
        let config = SpeculativeDecodingConfig {
            max_speculation_length: 4,
            min_speculation_length: 2,
            adaptive_speculation: false,
            ..Default::default()
        };

        let decoder = SpeculativeDecoder::new(config);
        let draft_model = MockDraftModel::new(vec![1, 2, 3, 4, 5, 6, 7, 8]);
        let target_model = MockTargetModel::new(0.75); // 75% acceptance rate

        let context = vec![100, 200, 300];
        let generated = decoder.generate(&context, 10, &draft_model, &target_model).await;

        assert!(generated.is_ok());
        let tokens = generated.unwrap();
        assert!(!tokens.is_empty());
        assert!(tokens.len() <= 10);
    }

    #[tokio::test]
    async fn test_high_acceptance_rate() {
        let config = SpeculativeDecodingConfig::default();
        let decoder = SpeculativeDecoder::new(config);
        let draft_model = MockDraftModel::new(vec![1, 2, 3, 4]);
        let target_model = MockTargetModel::new(1.0); // 100% acceptance rate

        let context = vec![100];
        let generated = decoder.generate(&context, 4, &draft_model, &target_model).await;

        assert!(generated.is_ok());
        let tokens = generated.unwrap();
        assert_eq!(tokens.len(), 4);
        assert_eq!(tokens, vec![1, 2, 3, 4]);

        let stats = decoder.get_stats().await;
        assert!(stats.total_accepted_tokens > 0);
        assert_eq!(stats.total_rejected_tokens, 0);
    }

    #[tokio::test]
    async fn test_low_acceptance_rate() {
        let config = SpeculativeDecodingConfig::default();
        let decoder = SpeculativeDecoder::new(config);
        let draft_model = MockDraftModel::new(vec![1, 2, 3, 4]);
        let target_model = MockTargetModel::new(0.0); // 0% acceptance rate

        let context = vec![100];
        let generated = decoder.generate(&context, 4, &draft_model, &target_model).await;

        assert!(generated.is_ok());
        let tokens = generated.unwrap();
        assert_eq!(tokens.len(), 4);
        // All tokens should be the correction token (9999)
        for token in tokens {
            assert_eq!(token, 9999);
        }

        let stats = decoder.get_stats().await;
        assert_eq!(stats.total_accepted_tokens, 0);
        assert!(stats.total_rejected_tokens > 0);
    }

    #[tokio::test]
    async fn test_speculative_decoding_manager() {
        let config = SpeculativeDecodingConfig::default();
        let manager = SpeculativeDecodingManager::new(config);

        let decoder1 = manager.get_decoder("session1").await;
        let decoder2 = manager.get_decoder("session2").await;
        let decoder1_again = manager.get_decoder("session1").await;

        // Should return the same decoder for the same session
        assert!(Arc::ptr_eq(&decoder1, &decoder1_again));

        // Should return different decoders for different sessions
        assert!(!Arc::ptr_eq(&decoder1, &decoder2));

        let all_stats = manager.get_all_stats().await;
        assert_eq!(all_stats.len(), 2);
        assert!(all_stats.contains_key("session1"));
        assert!(all_stats.contains_key("session2"));
    }

    #[tokio::test]
    async fn test_config_serialization() {
        let config = SpeculativeDecodingConfig {
            max_speculation_length: 6,
            target_acceptance_rate: 0.8,
            adaptive_speculation: true,
            ..Default::default()
        };

        let json = serde_json::to_string(&config).unwrap();
        let deserialized: SpeculativeDecodingConfig = serde_json::from_str(&json).unwrap();

        assert_eq!(
            config.max_speculation_length,
            deserialized.max_speculation_length
        );
        assert_eq!(
            config.target_acceptance_rate,
            deserialized.target_acceptance_rate
        );
        assert_eq!(
            config.adaptive_speculation,
            deserialized.adaptive_speculation
        );
    }

    #[tokio::test]
    async fn test_stats_serialization() {
        let stats = SpeculativeStats {
            total_sequences: 100,
            total_draft_tokens: 1000,
            total_accepted_tokens: 750,
            average_acceptance_rate: 0.75,
            ..Default::default()
        };

        let json = serde_json::to_string(&stats).unwrap();
        let deserialized: SpeculativeStats = serde_json::from_str(&json).unwrap();

        assert_eq!(stats.total_sequences, deserialized.total_sequences);
        assert_eq!(stats.total_draft_tokens, deserialized.total_draft_tokens);
        assert_eq!(
            stats.total_accepted_tokens,
            deserialized.total_accepted_tokens
        );
        assert_eq!(
            stats.average_acceptance_rate,
            deserialized.average_acceptance_rate
        );
    }
}
